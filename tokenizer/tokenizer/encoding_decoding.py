import json
import os
import tokenizer.utils.token_utils as tu


class Tokenizer:
    def __init__(self):
        # Guarantee order of ids when storing the merges
        self.merges = self._load_merges()
        self.vocab = self.generate_vocab()

    @staticmethod
    def _load_merges() -> dict[int, tuple[int, int]]:
        """
        Reads the json containing all merges that was generated by the training algorithm.

        Returns:
            merges: a dictionary with the token ids as keys and the token pair that merged to form said id
        """
        merges_file_path = os.path.abspath(os.path.join(
            __file__,
            '..', '..',
            'training_outputs',
            'merges.json'
        ))

        with open(merges_file_path) as file:
            merges = json.load(file)
        merges = dict(sorted(
            {int(k): v for k, v in merges.items()}.items(),
            key=lambda x: x[0]
        ))
        return merges

    def generate_vocab(self) -> dict[int, bytes]:
        """
        Aggregate on a dictionary all the possible tokens by including the base ones (0 to 255) provided by the utf-8
        representation of every character and the tokens created through merging in the training process.

        Returns:
            complete_vocab: all tokens identifiable by the tokenizer
        """
        vocab = {idx: bytes([idx]) for idx in range(256)}
        for idx, (base_id_0, base_id_1) in self.merges.items():
            vocab[int(idx)] = vocab[base_id_0] + vocab[base_id_1]
        return vocab

    def encode(self, text: str) -> list[int]:
        """
        Using the 'merges' dictionary, map a text from its string format to its token representation

        Args:
            text: original string to be mapped

        Returns:
            tokens: text as multiple individual tokens
        """

        # Easier for the encoding case, so one can map the pair to its token
        inverse_merges = {tuple(v): k for k, v in self.merges.items()}
        tokens = list(text.encode('utf-8'))
        while True:
            # Identify which pair contained in the string should be merged (smaller ids first)
            counts = tu.get_pair_counts(tokens)
            pair_to_merge = min(counts, key=lambda pair: inverse_merges.get(pair, float('inf')))
            if pair_to_merge not in inverse_merges:
                break
            else:
                tokens = tu.replace_with_new_token(
                    [tokens], pair_to_merge, inverse_merges[pair_to_merge]
                )[0]
        return tokens

    def decode(self, ids: list[int]) -> str:
        """
        Reverse process of encoding: turns tokens back into its original strings

        Args:
            ids: list of token ids

        Returns:

        """
        ids_to_bytes = b"".join(self.vocab[idx] for idx in ids)
        text = ids_to_bytes.decode('utf-8', errors='ignore')
        return text
